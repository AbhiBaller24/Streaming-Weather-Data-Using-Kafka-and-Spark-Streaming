{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de69dda-38af-4b47-9264-feb7edcad7f7",
   "metadata": {},
   "source": [
    "# Notebook Running Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036f710-572c-47e9-9122-3aafe1ada5c3",
   "metadata": {},
   "source": [
    "## Random Weather Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7946ae-0973-46fd-ae51-0b3b280d3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, random, string\n",
    "\n",
    "def one_station(name):\n",
    "    # temp pattern\n",
    "    month_avg = [27,31,44,58,70,79,83,81,74,61,46,32]\n",
    "    shift = (random.random()-0.5) * 30\n",
    "    month_avg = [m + shift + (random.random()-0.5) * 5 for m in month_avg]\n",
    "    \n",
    "    # rain pattern\n",
    "    start_rain = [0.1,0.1,0.3,0.5,0.4,0.2,0.2,0.1,0.2,0.2,0.2,0.1]\n",
    "    shift = (random.random()-0.5) * 0.1\n",
    "    start_rain = [r + shift + (random.random() - 0.5) * 0.2 for r in start_rain]\n",
    "    stop_rain = 0.2 + random.random() * 0.2\n",
    "\n",
    "    # day's state\n",
    "    today = datetime.date(2000, 1, 1)\n",
    "    temp = month_avg[0]\n",
    "    raining = False\n",
    "    \n",
    "    # gen weather\n",
    "    while True:\n",
    "        # choose temp+rain\n",
    "        month = today.month - 1\n",
    "        temp = temp * 0.8 + month_avg[month] * 0.2 + (random.random()-0.5) * 20\n",
    "        if temp < 32:\n",
    "            raining=False\n",
    "        elif raining and random.random() < stop_rain:\n",
    "            raining = False\n",
    "        elif not raining and random.random() < start_rain[month]:\n",
    "            raining = True\n",
    "\n",
    "        yield (today.strftime(\"%Y-%m-%d\"), name, temp, raining)\n",
    "\n",
    "        # next day\n",
    "        today += datetime.timedelta(days=1)\n",
    "        \n",
    "def all_stations(count=10, sleep_sec=1):\n",
    "    assert count <= 26\n",
    "    stations = []\n",
    "    for name in string.ascii_uppercase[:count]:\n",
    "        stations.append(one_station(name))\n",
    "    while True:\n",
    "        for station in stations:\n",
    "            yield next(station)\n",
    "        time.sleep(sleep_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4987a6-d283-44d5-8d1e-25cf7cee9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops forever because the weather never ends...\n",
    "#for row in all_stations(3):\n",
    "    #print(row) # date, station, temp, raining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6c513-b704-429a-bcc6-055308fb2c0d",
   "metadata": {},
   "source": [
    "## Kafka Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1748fbdd-ce1e-4b74-88ee-d2571213765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stations-json', 'stations', '__consumer_offsets']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer, TopicPartition\n",
    "from kafka.admin import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError, UnknownTopicOrPartitionError\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=[\"kafka:9092\"])\n",
    "try:\n",
    "    admin.delete_topics([\"stations\", \"stations-json\"])\n",
    "    print(\"deleted\")\n",
    "except UnknownTopicOrPartitionError:\n",
    "    print(\"cannot delete (may not exist yet)\")\n",
    "\n",
    "time.sleep(1)\n",
    "admin.create_topics([NewTopic(\"stations\", 6, 1)])\n",
    "admin.create_topics([NewTopic(\"stations-json\", 6, 1)])\n",
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e1eb6-747f-49e7-a112-2797195bd739",
   "metadata": {},
   "source": [
    "Building proto file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6d1bb6-da06-492b-a8b7-5f0b07f6579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m grpc_tools.protoc -I=. --python_out=. stations.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1e0f6-d501-499c-8411-6dc8369c0bd1",
   "metadata": {},
   "source": [
    "## Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb8285d-dce1-4149-9f02-d12efad4aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thread-safe print\n",
    "from threading import Thread, Lock\n",
    "import json\n",
    "\n",
    "lock = Lock()\n",
    "def Print(*args):\n",
    "    with lock:\n",
    "        print(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a9e13c-72fb-487b-ace0-3e11bcea2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stations_pb2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a980878d-6e0e-4f10-9d23-e7964f0491e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending station data to topics...\n"
     ]
    }
   ],
   "source": [
    "def produce():\n",
    "    Print(\"sending station data to topics...\")\n",
    "    #gaurantee us at-least-once semantics:\n",
    "    #we set acks to all, which sends back ack only if all data is safely commited aka all in-sync replicas have received it\n",
    "    #we set retries to 10 so we keep retrying until we get a strong ack back\n",
    "    #we write code in consumer to improve and get desired exactly-one-semantic\n",
    "    producer = KafkaProducer(bootstrap_servers=[\"kafka:9092\"], acks= \"all\", retries=10)\n",
    "    \n",
    "    for date, station, degrees, raining in all_stations(15):\n",
    "        key = bytes(station, \"utf-8\")\n",
    "        \n",
    "        #sending to stations topic\n",
    "        rep = Report(date = date, station = station, degrees = degrees, raining = raining)\n",
    "        value = rep.SerializeToString()\n",
    "        producer.send(\"stations\", value=value, key=key)\n",
    "        \n",
    "        #sending to stations-json topic\n",
    "        rainingBinary = 0\n",
    "        if raining == True:\n",
    "            rainingBinary = 1\n",
    "        else:\n",
    "            rainingBinary = 0\n",
    "            \n",
    "        value = {\"date\": date, \"station\": station, \"degrees\": degrees, \"raining\": rainingBinary}\n",
    "        value = bytes(json.dumps(value), \"utf-8\")\n",
    "        producer.send(\"stations-json\", value=value, key=key)\n",
    "        \n",
    "#start thread to run produce\n",
    "Thread(target = produce).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea5719-c56a-40fb-b78d-454dc1d7ad69",
   "metadata": {},
   "source": [
    "## Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "308b2887-10b8-4df5-ab21-ad961f788075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "#Delete json files if already exist\n",
    "for partition in range(6):\n",
    "    path = f\"partition-{partition}.json\"\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ff0218-5b76-439b-9573-f34e2685f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition(partition_num):\n",
    "    path = f\"partition-{partition_num}.json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        return {\"partition\": partition_num, \"offset\": 0}\n",
    "\n",
    "def save_partition(partition):\n",
    "    path = f\"partition-{partition['partition']}.json\"\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(partition, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85ae6da-a4a1-4672-a852-50a392ebeffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let the producer produce some reports\n",
    "time.sleep(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9897c6ca-5667-4338-a59a-9b9c8b7eb42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUND 0\n",
      "exiting\n",
      "exiting\n",
      "exiting\n",
      "ROUND 1\n",
      "exiting\n",
      "exiting\n",
      "exiting\n"
     ]
    }
   ],
   "source": [
    "def consume(part_nums=[], iterations=10):\n",
    "    consumer = KafkaConsumer(bootstrap_servers=[\"kafka:9092\"])\n",
    "    #list of TopicPartition objects\n",
    "    topicParts = [TopicPartition(\"stations\", num) for num in part_nums]\n",
    "    consumer.assign(topicParts)\n",
    "\n",
    "    # PART 1: initialization\n",
    "    partitions = {} # key=partition num, value=snapshot dict\n",
    "    #load partitions from JSON files (if they exist) or create fresh dicts\n",
    "    #if offsets were specified in previous JSON files, the consumer\n",
    "    #should seek to those; else, seek to offset 0.\n",
    "    for i in range(len(part_nums)):\n",
    "        partitions[part_nums[i]] = load_partition(part_nums[i])\n",
    "        if \"offset\" in partitions[part_nums[i]]:\n",
    "            consumer.seek(topicParts[i], partitions[part_nums[i]][\"offset\"])\n",
    "        else:\n",
    "            consumer.seek(topicParts[i], 0)\n",
    "    \n",
    "\n",
    "    # PART 2: process batches\n",
    "    for i in range(iterations):\n",
    "        batch = consumer.poll(1000) # 1s timeout\n",
    "        for topic, messages in batch.items():\n",
    "            #update the partitions based on new messages\n",
    "            #save the data back to the JSON file\n",
    "            pnum = topic.partition #getting the partition number\n",
    "            snapshot = partitions[pnum]\n",
    "            for msg in messages:\n",
    "                #deserializing:\n",
    "                s = Report.FromString(msg.value)\n",
    "                \n",
    "                #case when station in snapshot\n",
    "                if s.station in snapshot:\n",
    "                    if s.date > snapshot[s.station][\"end\"]:\n",
    "                        snapshot[s.station][\"count\"] += 1                        #incrementing count\n",
    "                        snapshot[s.station][\"sum\"] += s.degrees                  #updating sum of temps\n",
    "                        snapshot[s.station][\"end\"] = s.date                      #updating end date\n",
    "                        snapshot[s.station][\"avg\"] = snapshot[s.station][\"sum\"]/snapshot[s.station][\"count\"] #updating avg\n",
    "                        #snapshot[\"offset\"] += 1\n",
    "                 \n",
    "                #case when it is not\n",
    "                else:\n",
    "                    snapshot[s.station] = {}\n",
    "                    snapshot[s.station][\"count\"] = 1\n",
    "                    snapshot[s.station][\"sum\"] = s.degrees\n",
    "                    snapshot[s.station][\"avg\"] = s.degrees\n",
    "                    snapshot[s.station][\"start\"] = s.date\n",
    "                    snapshot[s.station][\"end\"] = s.date\n",
    "                    snapshot[s.station] = dict(sorted(snapshot[s.station].items())) #ordering\n",
    "                    #snapshot[\"offset\"] += 1\n",
    "            \n",
    "            snapshot[\"offset\"] = consumer.position(TopicPartition(\"stations\", pnum)) #setting appropriate offset \n",
    "            partitions[pnum] = dict(sorted(partitions[pnum].items())) #ordering\n",
    "    \n",
    "    #dumping data to json files\n",
    "    for p in partitions:\n",
    "        path = f\"partition-{p}.json\"\n",
    "        with open(path, \"w\") as file:\n",
    "            json.dump(partitions[p], file)\n",
    "    print(\"exiting\")\n",
    "\n",
    "#starting consumer threads\n",
    "for i in range(2):\n",
    "    print(\"ROUND\", i)\n",
    "    t1 = Thread(target=consume, args=([0,1], 30))\n",
    "    t2 = Thread(target=consume, args=([2,3], 30))\n",
    "    t3 = Thread(target=consume, args=([4,5], 30))\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d489645-bc75-4719-93c4-5739fed3e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"N\": {\"avg\": 45.73553359018883, \"count\": 139, \"end\": \"2000-05-18\", \"start\": \"2000-01-01\", \"sum\": 6357.239169036247}, \"offset\": 139, \"partition\": 0}{\"E\": {\"avg\": 46.032538477510876, \"count\": 139, \"end\": \"2000-05-18\", \"start\": \"2000-01-01\", \"sum\": 6398.522848374012}, \"O\": {\"avg\": 29.683402734456237, \"count\": 139, \"end\": \"2000-05-18\", \"start\": \"2000-01-01\", \"sum\": 4125.992980089417}, \"offset\": 278, \"partition\": 1}{\"F\": {\"avg\": 38.37035950738978, \"count\": 136, \"end\": \"2000-05-15\", \"start\": \"2000-01-01\", \"sum\": 5218.36889300501}, \"I\": {\"avg\": 55.505637464421795, \"count\": 136, \"end\": \"2000-05-15\", \"start\": \"2000-01-01\", \"sum\": 7548.766695161364}, \"J\": {\"avg\": 36.096625784804075, \"count\": 136, \"end\": \"2000-05-15\", \"start\": \"2000-01-01\", \"sum\": 4909.141106733354}, \"offset\": 408, \"partition\": 2}{\"D\": {\"avg\": 47.63517607643261, \"count\": 136, \"end\": \"2000-05-15\", \"start\": \"2000-01-01\", \"sum\": 6478.383946394834}, \"G\": {\"avg\": 43.2117556306084, \"count\": 136, \"end\": \"2000-05-15\", \"start\": \"2000-01-01\", \"sum\": 5876.798765762743}, \"M\": {\"avg\": 31.27415661681684, \"count\": 136, \"end\": \"2000-05-15\", \"start\": \"2000-01-01\", \"sum\": 4253.28529988709}, \"offset\": 408, \"partition\": 3}{\"A\": {\"avg\": 50.92114135378521, \"count\": 138, \"end\": \"2000-05-17\", \"start\": \"2000-01-01\", \"sum\": 7027.117506822359}, \"B\": {\"avg\": 59.978898649531445, \"count\": 138, \"end\": \"2000-05-17\", \"start\": \"2000-01-01\", \"sum\": 8277.088013635339}, \"C\": {\"avg\": 31.390467826751703, \"count\": 138, \"end\": \"2000-05-17\", \"start\": \"2000-01-01\", \"sum\": 4331.884560091735}, \"K\": {\"avg\": 37.594809141811474, \"count\": 138, \"end\": \"2000-05-17\", \"start\": \"2000-01-01\", \"sum\": 5188.083661569984}, \"L\": {\"avg\": 38.4487034492077, \"count\": 138, \"end\": \"2000-05-17\", \"start\": \"2000-01-01\", \"sum\": 5305.921075990663}, \"offset\": 690, \"partition\": 4}{\"H\": {\"avg\": 54.40097380246515, \"count\": 138, \"end\": \"2000-05-17\", \"start\": \"2000-01-01\", \"sum\": 7507.334384740191}, \"offset\": 138, \"partition\": 5}"
     ]
    }
   ],
   "source": [
    "!cat partition*.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
