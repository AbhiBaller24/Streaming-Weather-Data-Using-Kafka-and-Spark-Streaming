{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e21839-637a-4fcb-a22c-f3fc7abf3d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a2a313bc-0911-4981-a77c-4e824354b6cf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1080ms :: artifacts dl 45ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a2a313bc-0911-4981-a77c-4e824354b6cf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/27ms)\n",
      "23/04/30 01:04:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", 10)\n",
    "         .config(\"spark.ui.showConsoleProgress\", False)\n",
    "         .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2')\n",
    "         .getOrCreate())\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"stations-json\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8a96ed-c995-4d69-b624-811aa6a1957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json, date_add, month, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67673b-b1d5-458a-ad28-26f96b32bcda",
   "metadata": {},
   "source": [
    "### Part 3: Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5d1d96-ed42-4052-8122-3084fd137807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    " .option(\"subscribe\", \"stations-json\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78044493-e9cd-44c1-b660-b35efa27c36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4028f57f-7bb3-42fa-8180-f99513392e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bdc1374-ed98-4825-956f-741a9ef79ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key', 'binary'),\n",
       " ('value', 'binary'),\n",
       " ('topic', 'string'),\n",
       " ('partition', 'int'),\n",
       " ('offset', 'bigint'),\n",
       " ('timestamp', 'timestamp'),\n",
       " ('timestampType', 'int')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed02820-a6b7-464d-a17a-12c4004a2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "| key|               value|        topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|     0|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|     1|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|     2|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|     3|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|     4|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|     5|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|     6|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|     7|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|     8|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|     9|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|    10|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|    11|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|    12|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|    13|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|    14|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|    15|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|    16|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|    17|2023-04-30 00:56:...|            0|\n",
      "|[45]|[7B 22 64 61 74 6...|stations-json|        1|    18|2023-04-30 00:56:...|            0|\n",
      "|[4F]|[7B 22 64 61 74 6...|stations-json|        1|    19|2023-04-30 00:56:...|            0|\n",
      "+----+--------------------+-------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a85e3b-ba36-48c5-a17d-fb497b04f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+----------+-------+\n",
      "|key|      date|station|   degrees|raining|\n",
      "+---+----------+-------+----------+-------+\n",
      "|  E|2000-01-01|      E|  28.45602|      0|\n",
      "|  O|2000-01-01|      O|  27.26475|      0|\n",
      "|  E|2000-01-02|      E|  20.86555|      0|\n",
      "|  O|2000-01-02|      O|  20.83462|      0|\n",
      "|  E|2000-01-03|      E| 29.622593|      0|\n",
      "|  O|2000-01-03|      O|15.8072195|      0|\n",
      "|  E|2000-01-04|      E|  37.41072|      0|\n",
      "|  O|2000-01-04|      O| 28.667984|      0|\n",
      "|  E|2000-01-05|      E| 44.478657|      0|\n",
      "|  O|2000-01-05|      O| 21.241236|      0|\n",
      "|  E|2000-01-06|      E|  32.05573|      0|\n",
      "|  O|2000-01-06|      O| 14.483433|      0|\n",
      "|  E|2000-01-07|      E| 23.983406|      0|\n",
      "|  O|2000-01-07|      O| 25.098196|      0|\n",
      "|  E|2000-01-08|      E| 30.396551|      0|\n",
      "|  O|2000-01-08|      O| 20.354261|      0|\n",
      "|  E|2000-01-09|      E| 29.803612|      0|\n",
      "|  O|2000-01-09|      O| 28.349361|      0|\n",
      "|  E|2000-01-10|      E| 20.564102|      0|\n",
      "|  O|2000-01-10|      O| 23.384352|      0|\n",
      "+---+----------+-------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"date date, station string, degrees float, raining integer\"\n",
    "(df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "         .select(\"key\", \"value.*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d79ef8-23a3-4f56-8b68-b8747bf38b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.readStream.format(\"kafka\")\n",
    " .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    " .option(\"subscribe\", \"stations-json\")\n",
    " .option(\"startingOffsets\", \"earliest\").load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b070978-8f73-499d-a332-fcefd88177f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "station= (df.select(col(\"key\").cast(\"string\"),\n",
    "          from_json(col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "         .select(\"key\", \"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd043cb7-5f6c-4921-88d8-4590ad4a0956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d97894-4e91-441f-99d3-c6aa50f6b3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 01:05:04 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-08a89f13-e9df-41b6-bc2a-3fea3a2bd55d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/30 01:05:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      F|2000-01-01|2001-05-23|         509|45.224777637624086|  89.89394|\n",
      "|      K|2000-01-01|2001-05-23|         509|48.656605058194145|  91.99202|\n",
      "|      I|2000-01-01|2001-05-23|         509| 67.94582215929313| 114.58096|\n",
      "|      N|2000-01-01|2001-05-23|         509| 51.14132382897825| 97.028625|\n",
      "|      E|2000-01-01|2001-05-23|         509| 52.79992546501234| 100.50049|\n",
      "|      J|2000-01-01|2001-05-23|         509| 44.07271267224857|  88.68911|\n",
      "|      A|2000-01-01|2001-05-23|         509| 60.11602631439629| 114.12946|\n",
      "|      H|2000-01-01|2001-05-23|         509| 52.08816163619515|106.363304|\n",
      "|      G|2000-01-01|2001-05-23|         509| 38.81074190242234|  89.80698|\n",
      "|      M|2000-01-01|2001-05-23|         509| 66.62635135837998| 118.47488|\n",
      "|      L|2000-01-01|2001-05-23|         509| 51.70892614278437| 105.30565|\n",
      "|      D|2000-01-01|2001-05-23|         509| 58.45478897244617| 101.55333|\n",
      "|      O|2000-01-01|2001-05-23|         509| 59.51625803552114| 104.00182|\n",
      "|      C|2000-01-01|2001-05-23|         509| 66.78727422013499| 111.99958|\n",
      "|      B|2000-01-01|2001-05-23|         509|  65.5939097488793| 121.67864|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 01:05:14 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 9404 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      K|2000-01-01|2001-05-30|         516|48.999159538468646|  91.99202|\n",
      "|      F|2000-01-01|2001-05-30|         516| 45.42966654503992|  89.89394|\n",
      "|      I|2000-01-01|2001-05-30|         516| 68.28605990816456| 114.58096|\n",
      "|      N|2000-01-01|2001-05-30|         516| 51.25425077524296| 97.028625|\n",
      "|      E|2000-01-01|2001-05-30|         516| 52.90850942818693| 100.50049|\n",
      "|      J|2000-01-01|2001-05-30|         516|44.293935769057086|  88.68911|\n",
      "|      A|2000-01-01|2001-05-30|         516| 60.48156513169754| 114.12946|\n",
      "|      H|2000-01-01|2001-05-30|         516|52.407432743745254|106.363304|\n",
      "|      G|2000-01-01|2001-05-30|         516|38.998446699347376|  89.80698|\n",
      "|      M|2000-01-01|2001-05-30|         516| 66.88130583874015| 118.47488|\n",
      "|      L|2000-01-01|2001-05-30|         516|  51.8701233087584| 105.30565|\n",
      "|      D|2000-01-01|2001-05-30|         516| 58.68509643022404| 101.55333|\n",
      "|      O|2000-01-01|2001-05-30|         516| 59.73750805485156| 104.00182|\n",
      "|      C|2000-01-01|2001-05-30|         516| 66.88460256517395| 111.99958|\n",
      "|      B|2000-01-01|2001-05-30|         516| 65.70591479678487| 121.67864|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      K|2000-01-01|2001-06-02|         519| 49.16678013645385|  91.99202|\n",
      "|      F|2000-01-01|2001-06-02|         519| 45.54446206432776|  89.89394|\n",
      "|      I|2000-01-01|2001-06-02|         519| 68.47978262267361| 114.58096|\n",
      "|      N|2000-01-01|2001-06-02|         519| 51.31637156239364| 97.028625|\n",
      "|      E|2000-01-01|2001-06-02|         519|52.921440194337585| 100.50049|\n",
      "|      J|2000-01-01|2001-06-02|         519|44.468739468008565|  88.68911|\n",
      "|      A|2000-01-01|2001-06-02|         519|60.591573002251124| 114.12946|\n",
      "|      H|2000-01-01|2001-06-02|         519|52.539227754853826|106.363304|\n",
      "|      G|2000-01-01|2001-06-02|         519| 39.07196931902728|  89.80698|\n",
      "|      M|2000-01-01|2001-06-02|         519| 66.88610707174607| 118.47488|\n",
      "|      L|2000-01-01|2001-06-02|         519| 51.99790332772139| 105.30565|\n",
      "|      D|2000-01-01|2001-06-02|         519| 58.80233560315906| 101.55333|\n",
      "|      O|2000-01-01|2001-06-02|         519|59.791415887071906| 104.00182|\n",
      "|      C|2000-01-01|2001-06-02|         519| 66.92093674249962| 111.99958|\n",
      "|      B|2000-01-01|2001-06-02|         519| 65.79278502344856| 121.67864|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      K|2000-01-01|2001-06-04|         521| 49.24481473454129|  91.99202|\n",
      "|      F|2000-01-01|2001-06-04|         521| 45.61237857620913|  89.89394|\n",
      "|      I|2000-01-01|2001-06-04|         521| 68.63040813145848| 114.58096|\n",
      "|      N|2000-01-01|2001-06-04|         521| 51.32227959262204| 97.028625|\n",
      "|      E|2000-01-01|2001-06-04|         521| 52.99448612601194| 100.50049|\n",
      "|      J|2000-01-01|2001-06-04|         521| 44.57334894845673|  88.68911|\n",
      "|      A|2000-01-01|2001-06-04|         521|60.669715980192976| 114.12946|\n",
      "|      H|2000-01-01|2001-06-04|         521|  52.6486813724613|106.363304|\n",
      "|      G|2000-01-01|2001-06-04|         521| 39.15238670570036|  89.80698|\n",
      "|      M|2000-01-01|2001-06-04|         521| 66.87814620634873| 118.47488|\n",
      "|      L|2000-01-01|2001-06-04|         521|52.120812238521175| 105.30565|\n",
      "|      D|2000-01-01|2001-06-04|         521| 58.91747111139279| 101.55333|\n",
      "|      O|2000-01-01|2001-06-04|         521|59.835363089695086| 104.00182|\n",
      "|      C|2000-01-01|2001-06-04|         521| 66.98704415685613| 111.99958|\n",
      "|      B|2000-01-01|2001-06-04|         521| 65.90519722775626| 121.67864|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      K|2000-01-01|2001-06-09|         526| 49.46124255883829|  91.99202|\n",
      "|      F|2000-01-01|2001-06-09|         526| 45.83979617144218|  89.89394|\n",
      "|      I|2000-01-01|2001-06-09|         526| 68.92416089206594| 114.58096|\n",
      "|      N|2000-01-01|2001-06-09|         526| 51.41730754298403| 97.028625|\n",
      "|      E|2000-01-01|2001-06-09|         526| 53.19579309021112| 100.50049|\n",
      "|      J|2000-01-01|2001-06-09|         526| 44.83530175889853|  88.68911|\n",
      "|      A|2000-01-01|2001-06-09|         526| 60.79939198584611| 114.12946|\n",
      "|      H|2000-01-01|2001-06-09|         526|52.979569637276825|106.363304|\n",
      "|      G|2000-01-01|2001-06-09|         526|  39.4561002808322|  89.80698|\n",
      "|      M|2000-01-01|2001-06-09|         526|  66.9407725714912| 118.47488|\n",
      "|      L|2000-01-01|2001-06-09|         526|  52.3910667796552| 105.30565|\n",
      "|      D|2000-01-01|2001-06-09|         526|59.304467552968305| 104.80038|\n",
      "|      O|2000-01-01|2001-06-09|         526| 59.89456208486521| 104.00182|\n",
      "|      C|2000-01-01|2001-06-09|         526| 67.25330028969073| 111.99958|\n",
      "|      B|2000-01-01|2001-06-09|         526| 66.20588653259857| 121.67864|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|station|     start|       end|measurements|               avg|       max|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "|      K|2000-01-01|2001-06-14|         531| 49.73760908651262|  91.99202|\n",
      "|      F|2000-01-01|2001-06-14|         531| 46.01631641926738|  89.89394|\n",
      "|      I|2000-01-01|2001-06-14|         531| 69.10438060940098| 114.58096|\n",
      "|      N|2000-01-01|2001-06-14|         531| 51.65836838297476| 97.028625|\n",
      "|      E|2000-01-01|2001-06-14|         531|  53.2768069229557| 100.50049|\n",
      "|      J|2000-01-01|2001-06-14|         531| 45.11530690520946|  88.68911|\n",
      "|      A|2000-01-01|2001-06-14|         531| 60.95729317458561| 114.12946|\n",
      "|      H|2000-01-01|2001-06-14|         531|53.174572036764715|106.363304|\n",
      "|      G|2000-01-01|2001-06-14|         531| 39.70576644984904|  89.80698|\n",
      "|      M|2000-01-01|2001-06-14|         531| 67.00729882694682| 118.47488|\n",
      "|      L|2000-01-01|2001-06-14|         531| 52.60292212707175| 105.30565|\n",
      "|      D|2000-01-01|2001-06-14|         531|59.605856556003374| 104.80038|\n",
      "|      O|2000-01-01|2001-06-14|         531| 60.02641475806802| 104.00182|\n",
      "|      C|2000-01-01|2001-06-14|         531| 67.46677951489465| 111.99958|\n",
      "|      B|2000-01-01|2001-06-14|         531| 66.31764523996472| 121.67864|\n",
      "+-------+----------+----------+------------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 01:05:35 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3f4ca92b is aborting.\n",
      "23/04/30 01:05:35 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@3f4ca92b aborted.\n"
     ]
    }
   ],
   "source": [
    "counts_df = (station.select(station.date, station.station, station.degrees)\n",
    "             .groupby(\"station\")\n",
    "             .agg(\n",
    "                 expr(\"MIN(date)\").alias(\"start\"),\n",
    "                 expr(\"MAX(date)\").alias(\"end\"),\n",
    "                 expr(\"COUNT(degrees)\").alias(\"measurements\"),\n",
    "                 expr(\"AVG(degrees)\").alias(\"avg\"),\n",
    "                 expr(\"MAX(degrees)\").alias(\"max\")\n",
    "                     )\n",
    "            )\n",
    "\n",
    "s = counts_df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").outputMode(\"complete\").start()\n",
    "s.awaitTermination(30)\n",
    "s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa411f-8cfc-495d-aaa8-d20e3569606e",
   "metadata": {},
   "source": [
    "### Rain Forecast Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "084b4664-b61f-4318-bc5a-cf07c00ad6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 01:05:35 WARN TaskSetManager: Lost task 0.0 in stage 17.0 (TID 105) (81bf1d9b86f1 executor driver): TaskKilled (Stage cancelled)\n",
      "23/04/30 01:05:35 WARN TaskSetManager: Lost task 1.0 in stage 17.0 (TID 106) (81bf1d9b86f1 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, date: date, raining: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = (station.select(station.station, station.date, station.raining))\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02596d0-ff0c-464c-bfe0-03b267967ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf _spark_metadata/\n",
    "!rm -rf checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c921af-91c9-479d-b569-8f51d501ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 01:05:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "features = station.select(station.station, station.date, month(station.date).alias(\"month\"))\n",
    "features\n",
    "\n",
    "onePrior = (station.select(station.station, date_add(station.date, 1).alias(\"date\"),\n",
    "                           station.degrees.alias(\"sub1degrees\"), station.raining.alias(\"sub1raining\")))\n",
    "\n",
    "twoPrior = (station.select(station.station, date_add(station.date, 2).alias(\"date\"),\n",
    "                           station.degrees.alias(\"sub2degrees\"), station.raining.alias(\"sub2raining\")))\n",
    "# join the prior dataframes, then join to features  \n",
    "combined = onePrior.join(twoPrior,[\"station\",\"date\"])\n",
    "\n",
    "features = features.join(combined, [\"station\", \"date\"])\n",
    "\n",
    "# finally join features and today\n",
    "forecast = today.join(features, [\"station\", \"date\"])\n",
    "\n",
    "m = forecast.repartition(1).writeStream.format(\"parquet\").option(\"path\", \"/notebooks/parquet\").trigger(processingTime=\"1 minute\").option(\"checkpointLocation\", \"/notebooks/parquet\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420344ce-4bb6-4b46-86a1-f21e80a796a7",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd1dc7f6-5c95-47d6-8442-45f34255ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0767bf31-b5b6-49b7-874c-5d1f22138b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Parquet Files\n",
    "data = spark.read.parquet(\"/notebooks/parquet/part-00000-a2d1eaee-7e44-45f2-9557-cfadb5371aff-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c48158b-8dc6-443c-9447-2fabe70f582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub2degrees\", \"sub1raining\", \"sub2raining\"], outputCol=\"features\")\n",
    "data = va.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75ee9a1f-68c5-4eae-904d-cf504c822a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select([\"raining\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3af8292a-5bde-4cc2-ae5f-01e7745a825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e8f42f3-00b3-45b4-80d5-ef4d5d6f50fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(642, 123)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02d36a6b-9df7-4b11-a76e-2ee4964cc3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"raining\")\n",
    "dt_model = dt_classifier.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "970a41ef-8b31-47d7-8fcf-3c877a3ee2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_78b0560859be, depth=5, numNodes=35, numClasses=2, numFeatures=5\n",
      "  If (feature 3 <= 0.5)\n",
      "   If (feature 1 <= 31.847373008728027)\n",
      "    If (feature 4 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 4 > 0.5)\n",
      "     If (feature 1 <= 26.69634437561035)\n",
      "      If (feature 1 <= 24.737836837768555)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 24.737836837768555)\n",
      "       Predict: 1.0\n",
      "     Else (feature 1 > 26.69634437561035)\n",
      "      Predict: 0.0\n",
      "   Else (feature 1 > 31.847373008728027)\n",
      "    If (feature 2 <= 30.248023986816406)\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 > 30.248023986816406)\n",
      "     If (feature 2 <= 34.400983810424805)\n",
      "      If (feature 1 <= 33.586740493774414)\n",
      "       Predict: 1.0\n",
      "      Else (feature 1 > 33.586740493774414)\n",
      "       Predict: 0.0\n",
      "     Else (feature 2 > 34.400983810424805)\n",
      "      Predict: 0.0\n",
      "  Else (feature 3 > 0.5)\n",
      "   If (feature 4 <= 0.5)\n",
      "    If (feature 2 <= 45.63200759887695)\n",
      "     If (feature 2 <= 35.56413459777832)\n",
      "      If (feature 1 <= 32.59923553466797)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 32.59923553466797)\n",
      "       Predict: 1.0\n",
      "     Else (feature 2 > 35.56413459777832)\n",
      "      If (feature 1 <= 48.92373275756836)\n",
      "       Predict: 0.0\n",
      "      Else (feature 1 > 48.92373275756836)\n",
      "       Predict: 1.0\n",
      "    Else (feature 2 > 45.63200759887695)\n",
      "     Predict: 1.0\n",
      "   Else (feature 4 > 0.5)\n",
      "    If (feature 2 <= 35.56413459777832)\n",
      "     If (feature 0 <= 1.5)\n",
      "      If (feature 2 <= 32.41494369506836)\n",
      "       Predict: 0.0\n",
      "      Else (feature 2 > 32.41494369506836)\n",
      "       Predict: 1.0\n",
      "     Else (feature 0 > 1.5)\n",
      "      Predict: 0.0\n",
      "    Else (feature 2 > 35.56413459777832)\n",
      "     If (feature 1 <= 32.59923553466797)\n",
      "      Predict: 0.0\n",
      "     Else (feature 1 > 32.59923553466797)\n",
      "      Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dt_model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f904aea-992d-40e6-b279-d1c4f01c31ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy:  0.9225494438605919\n",
      "+-------------------+\n",
      "|       avg(raining)|\n",
      "+-------------------+\n",
      "|0.09411764705882353|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"raining\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Prediction Accuracy: \", accuracy)\n",
    "\n",
    "data.agg(avg(col(\"raining\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc5ddabe-cff0-46f9-b2e0-3a6ee9729b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = forecast.filter(col(\"station\") == \"A\") # gets us only station A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26420562-6fd3-48c4-981c-741f7338b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "va2 = VectorAssembler(inputCols=[\"month\", \"sub1degrees\", \"sub2degrees\", \"sub1raining\", \"sub2raining\"], outputCol = \"features\")\n",
    "va_data = va2.transform(filtered_data).select(\"station\", \"date\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6facafd1-00b8-43b9-9211-9cf4e0ab1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 01:05:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cee4da1d-6fb7-4b42-ad7b-784ede8777ac. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/30 01:05:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2000-01-10|       0.0|\n",
      "|      A|2000-01-23|       0.0|\n",
      "|      A|2000-01-26|       0.0|\n",
      "|      A|2000-02-29|       1.0|\n",
      "|      A|2000-03-12|       0.0|\n",
      "|      A|2000-03-18|       0.0|\n",
      "|      A|2000-03-21|       1.0|\n",
      "|      A|2000-05-15|       1.0|\n",
      "|      A|2000-05-16|       1.0|\n",
      "|      A|2000-05-20|       1.0|\n",
      "|      A|2000-05-22|       0.0|\n",
      "|      A|2000-05-29|       0.0|\n",
      "|      A|2000-06-16|       0.0|\n",
      "|      A|2000-06-24|       1.0|\n",
      "|      A|2000-06-27|       1.0|\n",
      "|      A|2000-07-09|       0.0|\n",
      "|      A|2000-07-25|       1.0|\n",
      "|      A|2000-08-03|       0.0|\n",
      "|      A|2000-08-05|       0.0|\n",
      "|      A|2000-09-11|       1.0|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2001-07-11|       1.0|\n",
      "|      A|2001-07-14|       1.0|\n",
      "|      A|2001-07-16|       0.0|\n",
      "|      A|2001-07-19|       1.0|\n",
      "|      A|2001-07-08|       1.0|\n",
      "|      A|2001-07-12|       1.0|\n",
      "|      A|2001-07-15|       0.0|\n",
      "|      A|2001-07-06|       1.0|\n",
      "|      A|2001-07-09|       1.0|\n",
      "|      A|2001-07-17|       0.0|\n",
      "|      A|2001-07-18|       0.0|\n",
      "|      A|2001-07-10|       1.0|\n",
      "|      A|2001-07-07|       1.0|\n",
      "|      A|2001-07-13|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2001-07-28|       1.0|\n",
      "|      A|2001-07-22|       0.0|\n",
      "|      A|2001-07-27|       0.0|\n",
      "|      A|2001-07-26|       0.0|\n",
      "|      A|2001-07-21|       0.0|\n",
      "|      A|2001-07-23|       1.0|\n",
      "|      A|2001-07-25|       1.0|\n",
      "|      A|2001-07-29|       1.0|\n",
      "|      A|2001-07-30|       1.0|\n",
      "|      A|2001-07-31|       1.0|\n",
      "|      A|2001-08-01|       0.0|\n",
      "|      A|2001-07-24|       0.0|\n",
      "|      A|2001-07-20|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-------+----------+----------+\n",
      "|station|      date|prediction|\n",
      "+-------+----------+----------+\n",
      "|      A|2001-08-12|       0.0|\n",
      "|      A|2001-08-10|       1.0|\n",
      "|      A|2001-08-11|       0.0|\n",
      "|      A|2001-08-13|       0.0|\n",
      "|      A|2001-08-07|       1.0|\n",
      "|      A|2001-08-08|       1.0|\n",
      "|      A|2001-08-14|       1.0|\n",
      "|      A|2001-08-15|       0.0|\n",
      "|      A|2001-08-04|       1.0|\n",
      "|      A|2001-08-05|       1.0|\n",
      "|      A|2001-08-02|       0.0|\n",
      "|      A|2001-08-03|       1.0|\n",
      "|      A|2001-08-06|       1.0|\n",
      "|      A|2001-08-16|       0.0|\n",
      "|      A|2001-08-09|       1.0|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/30 01:06:48 ERROR TorrentBroadcast: Store broadcast broadcast_497 fail, remove all pieces of the broadcast\n"
     ]
    }
   ],
   "source": [
    "final = dt_model.transform(va_data).select(\"station\", \"date\", \"prediction\").writeStream.format(\"console\").outputMode(\"append\").start()\n",
    "\n",
    "final.awaitTermination(60)\n",
    "final.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ee977-de52-4e3f-b052-0b2a3dea59a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
